*Web Crawler Application Overview*
"Backend (Node.js & Express)"
Web Crawling/Scraping:

Utilizes Gemini AI for efficient web crawling and scraping tasks.
Data is extracted from websites and processed as content using Cheerio, which loads and manipulates HTML and XML.
Database Management:

The application uses MongoDB as its database, with an ODM (Object Data Modeling) to interact with the database.
Security:

Sensitive data, including API keys and database credentials, is securely stored in environment variables (ENV files).
Analytics:

The backend provides an API endpoint for retrieving analytics related to the AI-powered web crawler.
Accessing Analytics: To view the analytics, you can use Postman or any REST client to make a GET request to the following URL:
https://web-crawler-6kqp.onrender.com/api/analytics
Deployment:

The backend is deployed on Render and can be accessed at:
https://web-crawler-6kqp.onrender.com


"Frontend (React)"
Frontend Framework:

The application uses React for building the frontend, taking advantage of its Single Page Application (SPA) capabilities for a seamless user experience.
Styling:

Material-UI is used to style various components and elements, providing a modern and responsive UI.
Frontend Deployment:

The frontend is deployed on Vercel and can be accessed at:
https://web-crawler-psi.vercel.app/
Setup Instructions
Cloning the Repository:

Use GitHub CI to clone the entire project repository.
Installing Dependencies:

Run npm install in both the frontend and backend directories to install all the necessary node_modules.
Running the Application Locally:

For the frontend:
Run npm start to launch the React application.
For the backend:
Run npm run dev to start the Node.js server in development mode.

Walk Through Video - https://drive.google.com/file/d/1sNzgjLqcJsrMROf4Gm1aUKkhNYOkD3we/view?usp=sharing